{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?\n",
    "# A1. R-squared, or the coefficient of determination, measures the proportion of variance in the dependent variable that is explained by the independent variables in a linear regression model. \n",
    "# It is calculated as:\n",
    "# R-squared = 1 - (SS_res / SS_tot),\n",
    "# where SS_res is the residual sum of squares and SS_tot is the total sum of squares.\n",
    "# A higher R-squared value indicates that the model explains a greater portion of the variance, meaning better fit to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
    "# A2. Adjusted R-squared is a modified version of R-squared that accounts for the number of predictors in the model. \n",
    "# While R-squared always increases when new variables are added, adjusted R-squared penalizes the model for including variables that do not improve model fit. \n",
    "# This makes it a more reliable metric for comparing models with different numbers of predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. When is it more appropriate to use adjusted R-squared?\n",
    "# A3. Adjusted R-squared is more appropriate when comparing models with different numbers of predictors. \n",
    "# It helps prevent overfitting by penalizing the inclusion of irrelevant variables, making it ideal for evaluating the true explanatory power of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?\n",
    "# A4. \n",
    "# - RMSE (Root Mean Square Error): This measures the square root of the average squared differences between predicted and actual values. It is calculated as:\n",
    "#   RMSE = sqrt(MSE).\n",
    "# - MSE (Mean Squared Error): This measures the average squared differences between predicted and actual values. It is calculated as:\n",
    "#   MSE = (1/n) * sum((y_pred - y_actual)^2).\n",
    "# - MAE (Mean Absolute Error): This measures the average absolute differences between predicted and actual values. It is calculated as:\n",
    "#   MAE = (1/n) * sum(|y_pred - y_actual|).\n",
    "# These metrics are used to evaluate the accuracy of regression models, with lower values indicating better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis.\n",
    "# A5. \n",
    "# Advantages:\n",
    "# - RMSE and MSE heavily penalize large errors, making them useful for applications where large deviations are critical.\n",
    "# - MAE is less sensitive to outliers, providing a robust measure of average error.\n",
    "# \n",
    "# Disadvantages:\n",
    "# - RMSE and MSE can overemphasize the impact of outliers.\n",
    "# - MAE does not penalize large errors as heavily, which might lead to underestimation of critical deviations.\n",
    "# The choice of metric depends on the specific use case and importance of outlier sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?\n",
    "# A6. Lasso regularization adds an L1 penalty, which is the absolute value of the coefficients, to the cost function. This encourages sparsity by shrinking some coefficients to exactly zero, effectively performing feature selection.\n",
    "# Ridge regularization, on the other hand, adds an L2 penalty, which is the square of the coefficients. It shrinks coefficients but does not set them to zero.\n",
    "# Lasso is more appropriate when feature selection is important, while Ridge is preferred when all predictors are relevant and multicollinearity needs to be addressed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate.\n",
    "# A7. Regularized linear models, such as Lasso and Ridge, add penalties to the cost function that reduce the magnitude of coefficients. This prevents overfitting by discouraging overly complex models that fit noise in the training data.\n",
    "# Example: In a dataset with many correlated predictors, Ridge regression reduces the impact of multicollinearity by shrinking coefficients, improving the model's generalization performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis.\n",
    "# A8. Limitations:\n",
    "# - Regularized models assume linear relationships and may not capture complex non-linear patterns.\n",
    "# - Lasso can arbitrarily exclude important predictors that are correlated with others.\n",
    "# - Ridge does not perform feature selection, making it less effective for sparse datasets.\n",
    "# Regularized models may not be ideal for datasets with strong non-linear relationships or when interpretability is a key concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?\n",
    "# A9. If robustness to outliers is a priority, Model B (MAE = 8) is preferred because MAE is less sensitive to large errors. However, RMSE penalizes large errors more heavily, so Model A might be better if minimizing significant deviations is crucial. The limitation is that each metric emphasizes different aspects of performance, so the choice depends on the specific application.\n",
    "\n",
    "# Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?\n",
    "# A10. The choice depends on the dataset:\n",
    "# - If feature selection is required, Model B (Lasso) is better because it shrinks some coefficients to zero.\n",
    "# - If multicollinearity is a concern and all predictors are relevant, Model A (Ridge) is preferable.\n",
    "# Trade-offs include Lasso potentially excluding important correlated predictors, while Ridge retains all predictors but does not perform feature selection. Regularization parameters should be fine-tuned using cross-validation for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
