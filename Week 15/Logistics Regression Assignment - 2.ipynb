{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. Purpose of Grid Search CV\n",
    "# Grid Search CV is used to find the optimal hyperparameters for a machine learning model.\n",
    "# It works by exhaustively searching over a specified parameter grid.\n",
    "# The performance of each combination is evaluated using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Q2. Difference between Grid Search CV and Randomized Search CV\n",
    "# Grid Search CV tests all possible combinations of hyperparameters.\n",
    "# This ensures finding the best combination but can be computationally expensive.\n",
    "# Randomized Search CV, on the other hand, samples random combinations.\n",
    "# It is faster and more efficient for larger parameter spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. What is Data Leakage?\n",
    "# Data leakage occurs when information outside the training dataset is used to create the model.\n",
    "# This can lead to overfitting and unrealistic performance metrics.\n",
    "# Example: Using future data or features derived from the target variable in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. Preventing Data Leakage\n",
    "# 1. Split data into train/test sets before preprocessing or feature selection.\n",
    "# 2. Apply preprocessing steps, such as scaling or encoding, only to the training data.\n",
    "# 3. Use the same transformations from training data on test data.\n",
    "# 4. Avoid including features that are directly correlated with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0]\n",
      " [1 3]]\n"
     ]
    }
   ],
   "source": [
    "# Q5. Confusion Matrix\n",
    "# A confusion matrix is a table that summarizes the performance of a classification model.\n",
    "# It displays counts of True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).\n",
    "\n",
    "# Example:\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Predictions and actual values\n",
    "y_true = [1, 0, 1, 1, 0, 1]\n",
    "y_pred = [1, 0, 1, 0, 0, 1]\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. Precision vs Recall\n",
    "# Precision measures the accuracy of positive predictions.\n",
    "# It is calculated as: Precision = TP / (TP + FP)\n",
    "# Recall measures the model's ability to capture positive cases.\n",
    "# It is calculated as: Recall = TP / (TP + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. Interpreting Confusion Matrix Errors\n",
    "# False Positives (FP): Cases that were predicted as positive but are actually negative.\n",
    "# These are also known as Type I errors.\n",
    "# False Negatives (FN): Cases that were predicted as negative but are actually positive.\n",
    "# These are also known as Type II errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80         2\n",
      "           1       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           0.83         6\n",
      "   macro avg       0.83      0.88      0.83         6\n",
      "weighted avg       0.89      0.83      0.84         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q8. Metrics Derived from Confusion Matrix\n",
    "# 1. Accuracy: Measures overall correctness.\n",
    "# Formula: Accuracy = (TP + TN) / (Total Samples)\n",
    "# 2. Precision: Measures the accuracy of positive predictions.\n",
    "# Formula: Precision = TP / (TP + FP)\n",
    "# 3. Recall: Measures the ability to capture all positive cases.\n",
    "# Formula: Recall = TP / (TP + FN)\n",
    "# 4. F1-Score: Harmonic mean of precision and recall.\n",
    "# Formula: F1-Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9. Relationship Between Accuracy and Confusion Matrix\n",
    "# Accuracy is calculated based on the values in the confusion matrix.\n",
    "# However, it may not be reliable for imbalanced datasets.\n",
    "# Metrics like Precision, Recall, and F1-Score are better indicators in such cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q10. Identifying Bias or Limitations\n",
    "# The confusion matrix can reveal class imbalances.\n",
    "# For example, high False Negatives may indicate poor performance for minority classes.\n",
    "# Additional metrics, like precision and recall, can help identify model biases.\n",
    "# It is also essential to analyze errors to determine whether the model needs improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
